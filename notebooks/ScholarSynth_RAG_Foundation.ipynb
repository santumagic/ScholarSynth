{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⬡ ScholarSynth | RAG Setup\n",
    "**Search. Synthesize. Succeed.**\n",
    "\n",
    "### 📋 Implementation Plan\n",
    "- Environment setup and imports\n",
    "- ArXiv API integration and paper retrieval\n",
    "- Text chunking implementation for academic papers\n",
    "- Qdrant vector database setup\n",
    "- Paper indexing and semantic search\n",
    "- Basic question-answering interface\n",
    "- Vibe checking and initial evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Environment Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# LangChain imports - UPDATED for newer LangChain versions\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ArXiv API\n",
    "import arxiv\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('✅ Environment setup complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieved 5 papers for query: \"machine learning transformer architecture\"\n",
      "\n",
      "📚 Sample Papers Retrieved:\n",
      "1. Differentiable Neural Architecture Transformation for Reproducible Architecture Improvement\n",
      "   Published: 2020-06-15 | Categories: cs.LG, cs.CV\n",
      "2. Transfer NAS: Knowledge Transfer between Search Spaces with Transformer Agents\n",
      "   Published: 2019-06-19 | Categories: cs.LG, stat.ML\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: ArXiv API Integration and Paper Retrieval\n",
    "class ArXivPaperRetriever:\n",
    "    \"\"\"\n",
    "    ArXiv paper retriever for academic research.\n",
    "    Implements AIE8 Week 2 concepts: Data collection and API integration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_results: int = 10):\n",
    "        self.max_results = max_results\n",
    "        \n",
    "    def search_papers(self, query: str, max_results: int = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search for papers on ArXiv based on query.\n",
    "        \n",
    "        Args:\n",
    "            query: Search query for papers\n",
    "            max_results: Maximum number of papers to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of paper dictionaries with metadata\n",
    "        \"\"\"\n",
    "        if max_results is None:\n",
    "            max_results = self.max_results\n",
    "            \n",
    "        try:\n",
    "            # Create ArXiv search\n",
    "            search = arxiv.Search(\n",
    "                query=query,\n",
    "                max_results=max_results,\n",
    "                sort_by=arxiv.SortCriterion.Relevance\n",
    "            )\n",
    "            \n",
    "            papers = []\n",
    "            for result in search.results():\n",
    "                paper = {\n",
    "                    'title': result.title,\n",
    "                    'authors': [author.name for author in result.authors],\n",
    "                    'abstract': result.summary,\n",
    "                    'published': result.published,\n",
    "                    'arxiv_id': result.entry_id.split('/')[-1],\n",
    "                    'categories': result.categories,\n",
    "                    'pdf_url': result.pdf_url,\n",
    "                    'doi': result.doi\n",
    "                }\n",
    "                papers.append(paper)\n",
    "                \n",
    "            print(f'✅ Retrieved {len(papers)} papers for query: \"{query}\"')\n",
    "            return papers\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'❌ Error retrieving papers: {e}')\n",
    "            return []\n",
    "\n",
    "# Initialize the retriever\n",
    "arxiv_retriever = ArXivPaperRetriever(max_results=5)\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "test_query = \"machine learning transformer architecture\"\n",
    "sample_papers = arxiv_retriever.search_papers(test_query)\n",
    "\n",
    "print(f\"\\n📚 Sample Papers Retrieved:\")\n",
    "for i, paper in enumerate(sample_papers[:2], 1):\n",
    "    print(f\"{i}. {paper['title']}\")\n",
    "    print(f\"   Published: {paper['published'].strftime('%Y-%m-%d')} | Categories: {', '.join(paper['categories'][:2])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Chunking Statistics:\n",
      "   Total chunks: 2\n",
      "   Avg length: 830 chars\n",
      "   Range: 815 - 844 chars\n",
      "\n",
      "📝 Sample Chunk Preview:\n",
      "   Title: Differentiable Neural Architecture Transformation for Reproducible Architecture Improvement\n",
      "\n",
      "Abstract: Recently, Neural Architecture Search (NA...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Text Chunking Implementation\n",
    "class AcademicTextSplitter:\n",
    "    \"\"\"\n",
    "    Text splitter optimized for academic papers.\n",
    "    Implements AIE8 Week 2 concepts: Document chunking and preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = CharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separator='\\n\\n'\n",
    "        )\n",
    "    \n",
    "    def split_paper(self, paper: Dict[str, Any]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Split a paper into chunks for vector storage.\n",
    "        \n",
    "        Args:\n",
    "            paper: Paper dictionary with title, abstract, etc.\n",
    "            \n",
    "        Returns:\n",
    "            List of Document objects\n",
    "        \"\"\"\n",
    "        # Combine title and abstract for chunking\n",
    "        full_text = f\"Title: {paper['title']}\\n\\nAbstract: {paper['abstract']}\"\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = self.text_splitter.split_text(full_text)\n",
    "        \n",
    "        # Create Document objects\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            doc = Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    'title': paper['title'],\n",
    "                    'authors': ', '.join(paper['authors'][:3]),\n",
    "                    'published': paper['published'].strftime('%Y-%m-%d'),\n",
    "                    'arxiv_id': paper['arxiv_id'],\n",
    "                    'categories': ', '.join(paper['categories'][:2]),\n",
    "                    'chunk_id': i,\n",
    "                    'total_chunks': len(chunks)\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def analyze_chunking(self, documents: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze chunking statistics.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with chunking statistics\n",
    "        \"\"\"\n",
    "        chunk_lengths = [len(doc.page_content) for doc in documents]\n",
    "        \n",
    "        stats = {\n",
    "            'total_chunks': len(documents),\n",
    "            'avg_chunk_length': np.mean(chunk_lengths),\n",
    "            'min_chunk_length': np.min(chunk_lengths),\n",
    "            'max_chunk_length': np.max(chunk_lengths),\n",
    "            'std_chunk_length': np.std(chunk_lengths)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = AcademicTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Test chunking with sample papers\n",
    "if sample_papers:\n",
    "    all_documents = []\n",
    "    for paper in sample_papers[:2]:  # Use first 2 papers for testing\n",
    "        docs = text_splitter.split_paper(paper)\n",
    "        all_documents.extend(docs)\n",
    "    \n",
    "    # Analyze chunking\n",
    "    chunking_stats = text_splitter.analyze_chunking(all_documents)\n",
    "    \n",
    "    print(f\"\\n📊 Chunking Statistics:\")\n",
    "    print(f\"   Total chunks: {chunking_stats['total_chunks']}\")\n",
    "    print(f\"   Avg length: {chunking_stats['avg_chunk_length']:.0f} chars\")\n",
    "    print(f\"   Range: {chunking_stats['min_chunk_length']:.0f} - {chunking_stats['max_chunk_length']:.0f} chars\")\n",
    "    \n",
    "    print(f\"\\n📝 Sample Chunk Preview:\")\n",
    "    doc = all_documents[0]\n",
    "    print(f\"   {doc.page_content[:150]}...\")\n",
    "else:\n",
    "    print(\"⚠️ No papers available for chunking\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initialized Qdrant vector database (production-ready)\n",
      "✅ Created Qdrant collection (1536-dim, cosine similarity)\n",
      "✅ Added 2 documents to Qdrant\n",
      "\n",
      "📊 Vector Store: 2 documents indexed (Qdrant)\n",
      "\n",
      "🔍 Top Search Results:\n",
      "1. Transfer NAS: Knowledge Transfer between Search Spaces with Transformer Agents... (Score: 0.406)\n",
      "2. Differentiable Neural Architecture Transformation for Reproducible Architecture ... (Score: 0.393)\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Qdrant Vector Database Setup\n",
    "# Upgraded from in-memory to production-ready Qdrant vector database\n",
    "# Implements AIE8 Week 2-3 concepts: Production vector databases and embeddings\n",
    "\n",
    "try:\n",
    "    from qdrant_client import QdrantClient\n",
    "    from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "    QDRANT_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"⚠️ Qdrant not installed. Run: pip install qdrant-client\")\n",
    "    print(\"   Falling back to in-memory vector store...\")\n",
    "    QDRANT_AVAILABLE = False\n",
    "\n",
    "class AcademicVectorStore:\n",
    "    \"\"\"\n",
    "    Production-ready vector store for academic papers using Qdrant.\n",
    "    Falls back to in-memory storage if Qdrant is not available.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"academic_papers\", use_qdrant: bool = True):\n",
    "        self.collection_name = collection_name\n",
    "        self.embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        self.use_qdrant = use_qdrant and QDRANT_AVAILABLE\n",
    "        \n",
    "        if self.use_qdrant:\n",
    "            # Initialize Qdrant (in-memory for demo, can connect to server)\n",
    "            self.qdrant_client = QdrantClient(\":memory:\")\n",
    "            # For production server: QdrantClient(host=\"localhost\", port=6333)\n",
    "            self.document_map = {}  # Store metadata separately\n",
    "            print(\"✅ Initialized Qdrant vector database (production-ready)\")\n",
    "        else:\n",
    "            # Fallback to in-memory\n",
    "            self.documents = []\n",
    "            self.embeddings_cache = []\n",
    "            print(\"✅ Initialized in-memory vector store (fallback mode)\")\n",
    "    \n",
    "    def create_collection(self):\n",
    "        \"\"\"Create a new collection for academic papers.\"\"\"\n",
    "        if self.use_qdrant:\n",
    "            try:\n",
    "                # Create Qdrant collection with proper configuration\n",
    "                self.qdrant_client.create_collection(\n",
    "                    collection_name=self.collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=1536,  # OpenAI text-embedding-3-small dimension\n",
    "                        distance=Distance.COSINE  # Cosine similarity for semantic search\n",
    "                    )\n",
    "                )\n",
    "                print(f\"✅ Created Qdrant collection (1536-dim, cosine similarity)\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error creating collection: {e}\")\n",
    "                return False\n",
    "        else:\n",
    "            print(f\"✅ Created collection '{self.collection_name}' (in-memory)\")\n",
    "            return True\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]) -> bool:\n",
    "        \"\"\"Add documents to the vector store.\"\"\"\n",
    "        if not documents:\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            # Generate embeddings\n",
    "            texts = [doc.page_content for doc in documents]\n",
    "            embeddings = self.embeddings.embed_documents(texts)\n",
    "            \n",
    "            if self.use_qdrant:\n",
    "                # Upload to Qdrant\n",
    "                points = []\n",
    "                for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "                    point_id = len(self.document_map)\n",
    "                    points.append(\n",
    "                        PointStruct(\n",
    "                            id=point_id,\n",
    "                            vector=embedding,\n",
    "                            payload={\n",
    "                                \"text\": doc.page_content,\n",
    "                                \"metadata\": doc.metadata\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "                    self.document_map[point_id] = doc\n",
    "                \n",
    "                self.qdrant_client.upsert(\n",
    "                    collection_name=self.collection_name,\n",
    "                    points=points\n",
    "                )\n",
    "                print(f\"✅ Added {len(documents)} documents to Qdrant\")\n",
    "            else:\n",
    "                # In-memory fallback\n",
    "                self.documents.extend(documents)\n",
    "                self.embeddings_cache.extend(embeddings)\n",
    "                print(f\"✅ Added {len(documents)} documents to in-memory store\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error adding documents: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def search_similar(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Search for similar documents.\"\"\"\n",
    "        try:\n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embeddings.embed_query(query)\n",
    "            \n",
    "            if self.use_qdrant:\n",
    "                # Search using Qdrant\n",
    "                search_results = self.qdrant_client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=query_embedding,\n",
    "                    limit=k\n",
    "                )\n",
    "                \n",
    "                # Format results\n",
    "                results = []\n",
    "                for hit in search_results:\n",
    "                    results.append({\n",
    "                        'text': hit.payload['text'],\n",
    "                        'score': hit.score,\n",
    "                        'metadata': hit.payload['metadata']\n",
    "                    })\n",
    "                return results\n",
    "                \n",
    "            else:\n",
    "                # In-memory fallback\n",
    "                if not self.documents:\n",
    "                    return []\n",
    "                    \n",
    "                similarities = []\n",
    "                for i, doc_embedding in enumerate(self.embeddings_cache):\n",
    "                    similarity = np.dot(query_embedding, doc_embedding) / (\n",
    "                        np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding)\n",
    "                    )\n",
    "                    similarities.append((i, similarity))\n",
    "                \n",
    "                similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "                top_k = similarities[:k]\n",
    "                \n",
    "                results = []\n",
    "                for idx, score in top_k:\n",
    "                    doc = self.documents[idx]\n",
    "                    results.append({\n",
    "                        'text': doc.page_content,\n",
    "                        'score': score,\n",
    "                        'metadata': doc.metadata\n",
    "                    })\n",
    "                return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error searching: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics about the vector store.\"\"\"\n",
    "        if self.use_qdrant:\n",
    "            try:\n",
    "                info = self.qdrant_client.get_collection(self.collection_name)\n",
    "                return {\n",
    "                    'total_documents': info.points_count,\n",
    "                    'vector_size': info.config.params.vectors.size,\n",
    "                    'distance_metric': info.config.params.vectors.distance.name,\n",
    "                    'backend': 'Qdrant'\n",
    "                }\n",
    "            except:\n",
    "                return {'backend': 'Qdrant', 'status': 'Error'}\n",
    "        else:\n",
    "            return {\n",
    "                'total_documents': len(self.documents),\n",
    "                'vector_size': 1536,\n",
    "                'distance_metric': 'COSINE',\n",
    "                'backend': 'In-Memory'\n",
    "            }\n",
    "\n",
    "# Initialize vector store with Qdrant\n",
    "vector_store = AcademicVectorStore(use_qdrant=True)\n",
    "\n",
    "# Create collection and add documents\n",
    "if vector_store.create_collection():\n",
    "    if 'all_documents' in locals() and all_documents:\n",
    "        vector_store.add_documents(all_documents)\n",
    "        \n",
    "        # Display statistics\n",
    "        stats = vector_store.get_stats()\n",
    "        print(f\"\\n📊 Vector Store: {stats['total_documents']} documents indexed ({stats['backend']})\")\n",
    "        \n",
    "        # Test search\n",
    "        test_query = \"transformer architecture attention mechanism\"\n",
    "        search_results = vector_store.search_similar(test_query, k=2)\n",
    "        \n",
    "        print(f\"\\n🔍 Top Search Results:\")\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            print(f\"{i}. {result['metadata'].get('title', 'N/A')[:80]}... (Score: {result['score']:.3f})\")\n",
    "    else:\n",
    "        print(\"⚠️ No documents available to add to vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Testing ScholarSynth Q&A:\n",
      "\n",
      "Q1: What is a transformer architecture?\n",
      "A: The transformer architecture is not explicitly defined in the provided research context. However, it can be inferred that it is a type of model or agent used in Neural Architecture Search (NAS) method...\n",
      "Confidence: 0.93 | Sources: 2\n",
      "   [1] Differentiable Neural Architecture Transformation for Reprod... (Score: 0.463)\n",
      "   [2] Transfer NAS: Knowledge Transfer between Search Spaces with ... (Score: 0.463)\n",
      "\n",
      "Q2: How does attention mechanism work?\n",
      "A: The research context does not provide information on how the attention mechanism works....\n",
      "Confidence: 0.42 | Sources: 2\n",
      "   [1] Transfer NAS: Knowledge Transfer between Search Spaces with ... (Score: 0.212)\n",
      "   [2] Differentiable Neural Architecture Transformation for Reprod... (Score: 0.210)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Basic Question-Answering Interface\n",
    "class AcademicResearchAssistant:\n",
    "    \"\"\"\n",
    "    Basic RAG-based research assistant.\n",
    "    Implements AIE8 Week 3 concepts: End-to-end RAG pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: AcademicVectorStore):\n",
    "        self.vector_store = vector_store\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1)\n",
    "        \n",
    "    def ask_question(self, question: str, k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Answer a research question using RAG.\"\"\"\n",
    "        try:\n",
    "            # Retrieve relevant documents\n",
    "            relevant_docs = self.vector_store.search_similar(question, k=k)\n",
    "            \n",
    "            if not relevant_docs:\n",
    "                return {\n",
    "                    'answer': 'I could not find relevant information to answer your question.',\n",
    "                    'sources': [],\n",
    "                    'confidence': 0.0\n",
    "                }\n",
    "            \n",
    "            # Prepare context for LLM\n",
    "            context = '\\n\\n'.join([doc['text'] for doc in relevant_docs])\n",
    "            \n",
    "            # Create prompt\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following academic research context, please answer the question.\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Please provide a comprehensive answer based on the research context. Include specific details and cite relevant information.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get answer from LLM\n",
    "            response = self.llm.invoke(prompt)\n",
    "            answer = response.content\n",
    "            \n",
    "            # Calculate confidence based on similarity scores\n",
    "            avg_score = np.mean([doc['score'] for doc in relevant_docs])\n",
    "            confidence = min(avg_score * 2, 1.0)\n",
    "            \n",
    "            # Prepare sources\n",
    "            sources = []\n",
    "            for doc in relevant_docs:\n",
    "                sources.append({\n",
    "                    'title': doc['metadata'].get('title', 'Unknown'),\n",
    "                    'authors': doc['metadata'].get('authors', 'Unknown'),\n",
    "                    'published': doc['metadata'].get('published', 'Unknown'),\n",
    "                    'arxiv_id': doc['metadata'].get('arxiv_id', 'Unknown'),\n",
    "                    'relevance_score': doc['score']\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'answer': answer,\n",
    "                'sources': sources,\n",
    "                'confidence': confidence,\n",
    "                'num_sources': len(sources)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error answering question: {e}\")\n",
    "            return {\n",
    "                'answer': f\"I encountered an error while processing your question: {e}\",\n",
    "                'sources': [],\n",
    "                'confidence': 0.0\n",
    "            }\n",
    "\n",
    "# Initialize the research assistant\n",
    "# Check if vector store has data (works for both Qdrant and in-memory modes)\n",
    "has_data = False\n",
    "if hasattr(vector_store, 'use_qdrant') and vector_store.use_qdrant:\n",
    "    # For Qdrant mode\n",
    "    has_data = len(vector_store.document_map) > 0\n",
    "elif hasattr(vector_store, 'documents'):\n",
    "    # For in-memory mode\n",
    "    has_data = len(vector_store.documents) > 0\n",
    "\n",
    "if has_data:\n",
    "    research_assistant = AcademicResearchAssistant(vector_store)\n",
    "    \n",
    "    # Test with sample questions\n",
    "    test_questions = [\n",
    "        \"What is a transformer architecture?\",\n",
    "        \"How does attention mechanism work?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🤖 Testing ScholarSynth Q&A:\\n\")\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"Q{i}: {question}\")\n",
    "        \n",
    "        result = research_assistant.ask_question(question)\n",
    "        \n",
    "        print(f\"A: {result['answer'][:200]}...\")\n",
    "        print(f\"Confidence: {result['confidence']:.2f} | Sources: {result['num_sources']}\")\n",
    "        \n",
    "        if result['sources']:\n",
    "            for j, source in enumerate(result['sources'][:2], 1):\n",
    "                print(f\"   [{j}] {source['title'][:60]}... (Score: {source['relevance_score']:.3f})\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"❌ Vector store not available. Please run the previous cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running Vibe Check...\n",
      "Testing 5 questions\n",
      "\n",
      "[1/5] What is machine learning?...\n",
      "      1039 chars | Confidence: 0.46 | 5.76s | ✅\n",
      "[2/5] How do neural networks work?...\n",
      "      78 chars | Confidence: 0.73 | 1.08s | ✅\n",
      "[3/5] What are the applications of AI?...\n",
      "      1515 chars | Confidence: 0.59 | 8.55s | ✅\n",
      "[4/5] What is deep learning?...\n",
      "      1655 chars | Confidence: 0.59 | 7.97s | ✅\n",
      "[5/5] How does natural language processing work?...\n",
      "      91 chars | Confidence: 0.59 | 2.47s | ✅\n",
      "\n",
      "============================================================\n",
      "🎯 VIBE CHECK SUMMARY\n",
      "============================================================\n",
      "Success Rate: 100.0% (5/5)\n",
      "Average Confidence: 0.59\n",
      "Average Response Time: 5.17s\n",
      "\n",
      "🎉 EXCELLENT! System is performing well.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Vibe Checking and Initial Evaluation\n",
    "import time\n",
    "\n",
    "class VibeChecker:\n",
    "    \"\"\"\n",
    "    Vibe checker for evaluating the research assistant.\n",
    "    Implements AIE8 Week 1 concepts: System evaluation and baseline establishment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, research_assistant: AcademicResearchAssistant):\n",
    "        self.research_assistant = research_assistant\n",
    "        \n",
    "    def run_vibe_check(self, test_questions: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive vibe check on the research assistant.\"\"\"\n",
    "        results = {\n",
    "            'total_questions': len(test_questions),\n",
    "            'successful_answers': 0,\n",
    "            'avg_confidence': 0.0,\n",
    "            'avg_sources': 0.0,\n",
    "            'response_times': [],\n",
    "            'detailed_results': []\n",
    "        }\n",
    "        \n",
    "        print(\"🔍 Running Vibe Check...\")\n",
    "        print(f\"Testing {len(test_questions)} questions\\n\")\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            start_time = time.time()\n",
    "            result = self.research_assistant.ask_question(question)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            response_time = end_time - start_time\n",
    "            results['response_times'].append(response_time)\n",
    "            \n",
    "            # Check if answer is meaningful\n",
    "            is_meaningful = (\n",
    "                len(result['answer']) > 50 and \n",
    "                'error' not in result['answer'].lower() and\n",
    "                result['confidence'] > 0.1\n",
    "            )\n",
    "            \n",
    "            if is_meaningful:\n",
    "                results['successful_answers'] += 1\n",
    "            \n",
    "            results['avg_confidence'] += result['confidence']\n",
    "            results['avg_sources'] += result['num_sources']\n",
    "            \n",
    "            detailed_result = {\n",
    "                'question': question,\n",
    "                'answer_length': len(result['answer']),\n",
    "                'confidence': result['confidence'],\n",
    "                'sources': result['num_sources'],\n",
    "                'response_time': response_time,\n",
    "                'is_meaningful': is_meaningful\n",
    "            }\n",
    "            results['detailed_results'].append(detailed_result)\n",
    "            \n",
    "            print(f\"[{i}/{len(test_questions)}] {question[:50]}...\")\n",
    "            print(f\"      {len(result['answer'])} chars | Confidence: {result['confidence']:.2f} | {response_time:.2f}s | {'✅' if is_meaningful else '❌'}\")\n",
    "        \n",
    "        # Calculate averages\n",
    "        results['avg_confidence'] /= len(test_questions)\n",
    "        results['avg_sources'] /= len(test_questions)\n",
    "        results['avg_response_time'] = np.mean(results['response_times'])\n",
    "        results['success_rate'] = results['successful_answers'] / len(test_questions)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_summary(self, results: Dict[str, Any]):\n",
    "        \"\"\"Print vibe check summary.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎯 VIBE CHECK SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Success Rate: {results['success_rate']:.1%} ({results['successful_answers']}/{results['total_questions']})\")\n",
    "        print(f\"Average Confidence: {results['avg_confidence']:.2f}\")\n",
    "        print(f\"Average Response Time: {results['avg_response_time']:.2f}s\")\n",
    "        \n",
    "        # Overall assessment\n",
    "        if results['success_rate'] >= 0.8 and results['avg_confidence'] >= 0.5:\n",
    "            print(\"\\n🎉 EXCELLENT! System is performing well.\")\n",
    "        elif results['success_rate'] >= 0.6 and results['avg_confidence'] >= 0.3:\n",
    "            print(\"\\n✅ GOOD! System working but could improve.\")\n",
    "        else:\n",
    "            print(\"\\n⚠️ NEEDS IMPROVEMENT! System needs optimization.\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Run vibe check if research assistant is available\n",
    "if 'research_assistant' in locals() and research_assistant is not None:\n",
    "    vibe_checker = VibeChecker(research_assistant)\n",
    "    \n",
    "    # Test questions for vibe check\n",
    "    test_questions = [\n",
    "        \"What is machine learning?\",\n",
    "        \"How do neural networks work?\",\n",
    "        \"What are the applications of AI?\",\n",
    "        \"What is deep learning?\",\n",
    "        \"How does natural language processing work?\"\n",
    "    ]\n",
    "    \n",
    "    # Run the vibe check\n",
    "    vibe_results = vibe_checker.run_vibe_check(test_questions)\n",
    "    \n",
    "    # Print summary\n",
    "    vibe_checker.print_summary(vibe_results)\n",
    "else:\n",
    "    print(\"❌ Research assistant not available. Please run Cell 5 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achievements:\n",
    "1. ✅ **Environment Setup** - All dependencies and imports configured\n",
    "2. ✅ **ArXiv Integration** - Academic paper retrieval working\n",
    "3. ✅ **Text Chunking** - Optimized for academic papers (1000 chars, 200 overlap)\n",
    "4. ✅ **Qdrant Vector Database** - Production-ready vector store with Qdrant (with in-memory fallback)\n",
    "5. ✅ **Semantic Search** - Cosine similarity search with OpenAI embeddings (text-embedding-3-small)\n",
    "6. ✅ **Basic RAG** - Question-answering pipeline working\n",
    "7. ✅ **Vibe Check** - Initial evaluation completed\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
